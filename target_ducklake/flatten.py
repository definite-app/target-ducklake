""" 
Adapted from target-duckdb
https://github.com/jwills/target-duckdb/blob/36c8ce68a0b2584c4bbb07325482968b1edc0c40/target_duckdb/db_sync.py#L74
"""
import itertools
import json
import collections
from decimal import Decimal

TIMESTAMP_COLUMN_NAMES = {
    "timestamp", "created_at", "updated_at", "deleted_at", "modified_at",
    "last_modified", "updatedat", "createdat"
}

class CustomJSONEncoder(json.JSONEncoder):
    # sometimes Python Decimal objects are returned, we need to convert them to floats when flattening the schema
    def default(self, obj):
        if isinstance(obj, Decimal):
            return float(obj)  # Convert Decimal to float
        return super().default(obj)

def flatten_key(k, parent_key, sep):
    return sep.join(parent_key + [k])

def _should_auto_cast_to_timestamp(column_name: str, schema_entry: dict, auto_cast_timestamps: bool) -> bool:
    """Check if a column should be auto-cast to timestamp format."""
    if not auto_cast_timestamps:
        return False
    
    # Skip if format is already specified
    if schema_entry.get("format") is not None:
        return False
    
    # Check if column name matches timestamp patterns
    if column_name.lower() not in TIMESTAMP_COLUMN_NAMES:
        return False
    
    # Check if it's a string type (either single string or list containing string)
    entry_type = schema_entry.get("type", [])
    if isinstance(entry_type, str):
        return entry_type == "string"
    elif isinstance(entry_type, list):
        return "string" in entry_type
    
    return False

def _apply_timestamp_format(schema_entry: dict) -> dict:
    """Apply timestamp format to a schema entry."""
    result = dict(schema_entry)  # Create a copy
    result["format"] = "date-time"
    return result


# pylint: disable-msg=too-many-arguments
def flatten_record(
    d, flatten_schema=None, parent_key=[], sep="__", level=0, max_level=0
):
    items = []
    for k, v in d.items():
        new_key = flatten_key(k, parent_key, sep)
        if isinstance(v, collections.abc.MutableMapping) and level < max_level:
            items.extend(
                flatten_record(
                    v,
                    flatten_schema,
                    parent_key + [k],
                    sep=sep,
                    level=level + 1,
                    max_level=max_level,
                ).items()
            )
        else:
            items.append(
                (
                    new_key,
                    json.dumps(v, cls=CustomJSONEncoder) # use custom encoder to convert Decimal type to float
                    if _should_json_dump_value(k, v, flatten_schema)
                    else v,
                )
            )
    return dict(items)


# pylint: disable=dangerous-default-value,invalid-name
def flatten_schema(d, parent_key=[], sep="__", level=0, max_level=0, auto_cast_timestamps=False):
    items = []

    if "properties" not in d:
        return {}

    for k, v in d["properties"].items():
        new_key = flatten_key(k, parent_key, sep)
        # some schemas from airbyte sources have description field which is not needed and causes issues
        if "description" in v.keys():
            del v["description"]
        
        if "type" in v.keys():
            if "object" in v["type"] and "properties" in v and level < max_level:
                items.extend(
                    flatten_schema(
                        v,
                        parent_key + [k],
                        sep=sep,
                        level=level + 1,
                        max_level=max_level,
                        auto_cast_timestamps=auto_cast_timestamps,
                    ).items()
                )
            # certain taps (e.g. postgres) sometimes return a list of types for JSON columns
            # we need to handle this specific case, if we don't the schema returns VARCHAR
            # may need to revisit this for other types
            elif "array" in v["type"]:
                items.append((new_key, {"type": ["null", "array"]}))
            elif "object" in v["type"]:
                items.append((new_key, {"type": ["null", "object"]}))
            else:
                # Apply timestamp auto-casting if applicable
                if _should_auto_cast_to_timestamp(new_key, v, auto_cast_timestamps):
                    items.append((new_key, _apply_timestamp_format(v)))
                else:
                    items.append((new_key, v))
        elif "anyOf" in v.keys():
            llv = v["anyOf"]
            i, entry = 0, llv[0]
            while entry["type"] == "null":
                i += 1
                entry = llv[i]
            entry["type"] = ["null", entry["type"]]
            items.append((new_key, entry))
        else:
            if len(v.values()) > 0:
                if list(v.values())[0][0]["type"] == "string":
                    list(v.values())[0][0]["type"] = ["null", "string"]
                    items.append((new_key, list(v.values())[0][0]))
                elif list(v.values())[0][0]["type"] == "array":
                    list(v.values())[0][0]["type"] = ["null", "array"]
                    items.append((new_key, list(v.values())[0][0]))
                elif list(v.values())[0][0]["type"] == "object":
                    list(v.values())[0][0]["type"] = ["null", "object"]
                    items.append((new_key, list(v.values())[0][0]))

    key_func = lambda item: item[0]
    sorted_items = sorted(items, key=key_func)
    for k, g in itertools.groupby(sorted_items, key=key_func):
        if len(list(g)) > 1:
            raise ValueError("Duplicate column name produced in schema: {}".format(k))

    return dict(sorted_items)


# pylint: disable=redefined-outer-name
def _should_json_dump_value(key, value, flatten_schema=None):
    if isinstance(value, (dict, list)):
        return True

    if (
        flatten_schema
        and key in flatten_schema
        and "type" in flatten_schema[key]
        and set(flatten_schema[key]["type"]) == {"null", "object", "array"}
    ):
        return True

    return False
